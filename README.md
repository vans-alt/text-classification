ğŸ“¦ Text Classification with Transformers

ğŸ§  Overview

This project implements a binary text classification pipeline using Hugging Face Transformers and the IMDB dataset. The goal is to fine-tune a pre-trained transformer (DistilBERT) to classify movie reviews as **positive** or **negative**.

ğŸ“ Project Structure

â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ text-classification.ipynb       # Full pipeline execution notebook   
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train_model.py                  # Model training using Hugging Face Trainer
â”‚   â”œâ”€â”€ data_preprocessing.py           # Text cleaning, tokenization, dataset prep
â”‚   â”œâ”€â”€ model_utils.py                  # Model loading/saving and pipeline use
â”‚   â””â”€â”€ config.py                       # All hyperparameters and model paths
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ trained_model/                  # Fine-tuned model weights
â”‚   â”œâ”€â”€ tokenizer/                      # Tokenizer files
â”‚   â””â”€â”€ checkpoints/                    # Intermediate training checkpoints
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ submission.md                   # Full write-up of approach & learnings
â”‚   â”œâ”€â”€ execution_report.md             # Execution steps + screenshots
â”‚   â”œâ”€â”€ evaluation_metrics.json         # Accuracy, F1, Precision, Recall
â”‚   â””â”€â”€ training_curve.png              # Graph of training loss vs. eval F1
â”‚
â”œâ”€â”€ train.py                            # Entry point to run full training
â”œâ”€â”€ requirements.txt                    # Python dependencies
â””â”€â”€ README.md                           # This file


ğŸ› ï¸ Setup

1. Clone this repository:

```bash
git clone <repo-url>
cd text-classification-pipeline
```

2. (Optional) Create a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
```

3. Install dependencies:

```bash
pip install -r requirements.txt
```


ğŸš€ How to Run

ğŸ“Œ Option 1: From notebook

Run `notebooks/text-classification.ipynb` in Google Colab or Jupyter Notebook for full pipeline.

ğŸ“Œ Option 2: From script

```bash
python train.py
```

ğŸ“Š Evaluation

| Metric    | Score  |
| --------- | -------|
| Accuracy  | ~0.905 |
| F1 Score  | ~0.906 |
| Precision | ~0.905 |
| Recall    | ~0.905 |

ğŸ“Œ Visualizations available in `reports/train loss, eval f1 plot.png`

ğŸ” Key Features

* Modular architecture (data, model, config separated)
* Easy experimentation with `Trainer`
* WANDB logging integrated
* Intermediate checkpoints and best model saving
* Ready to extend for multilingual classification (e.g. XLM-Roberta)


âœï¸ Author

Vanshita Gupta

ğŸª„ License

Open-source for educational purposes. Modify and reuse with attribution.

Happy fine-tuning! ğŸ¤—
"# text-classification" 
